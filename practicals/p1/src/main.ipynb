{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDqDj4kC9sFJ"
   },
   "source": [
    "# Master course in Object Recognition\n",
    "## Practice 1\n",
    "\n",
    "### Title: Deep learning advanced architectures\n",
    "\n",
    "The goal is to practice advanced deep learning architectures for multi-label classification in [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html). We specifically check ResNet50, Inception and MobileNet. We will see 1) how pretrained ResNet50 on imagenet performs on multi-label images, 2) how to modify classification head and 3) implementation of F1 metric.\n",
    "\n",
    "### NOTES\n",
    "\n",
    "- Hyperparameters are modifiable,\n",
    "- The dataset is PASCAL VOC 2012,\n",
    "- The code uses the KERAS library,\n",
    "- The code can run in google colab.\n",
    "- How to finetune on a pretrained model not included (i.e. freeze the pretrained network and train the head, then finetune everything),\n",
    "- No validation set has been defined. The test and validation sets are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    ")\n",
    "import tensorflow.keras.applications as app\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing from .py files\n",
    "\n",
    "from metrics import f1_metric, mean_average_precision, subset_accuracy_metric\n",
    "from config import *\n",
    "from experiment_config import experiments\n",
    "from train_and_test import train_and_test\n",
    "from load_data import load_data, create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create datasets: 0.9121880531311035 seconds\n"
     ]
    }
   ],
   "source": [
    "train_list = load_data(TRAIN_TXT)\n",
    "test_list = load_data(TEST_TXT)\n",
    "\n",
    "# Create dictionaries to store datasets for different batch sizes\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "start_time = time.time()\n",
    "# Iterate over batch sizes and create datasets\n",
    "for batch_size in BATCH_SIZES:\n",
    "    train_datasets[batch_size] = create_dataset(\n",
    "        train_list, batch_size, is_training=True\n",
    "    )\n",
    "    test_datasets[batch_size] = create_dataset(test_list, batch_size, is_training=False)\n",
    "print(f\"Time taken to create datasets: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8ej9VbIk_Sfj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model: resnet50 no-pretraining no-warmup\n",
      "In training loop: resnet50 no-pretraining no-warmup\n",
      "Time taken for training one epoch: 39.18s\n",
      "Epoch 0 training loss: 0.46, acc: 0.65, f1: 0.27, mAP: 0.48\n",
      "Time taken for testing one epoch: 7.48s\n",
      "Epoch 0 test loss: 0.62, acc: 0.47, f1: 0.05, mAP: 0.18\n",
      "Time taken for training one epoch: 31.92s\n",
      "Epoch 1 training loss: 0.27, acc: 0.73, f1: 0.33, mAP: 0.57\n",
      "Time taken for testing one epoch: 6.29s\n",
      "Epoch 1 test loss: 1.08, acc: 0.48, f1: 0.05, mAP: 0.19\n",
      "Time taken for training one epoch: 36.02s\n",
      "Epoch 2 training loss: 0.31, acc: 0.82, f1: 0.58, mAP: 0.77\n",
      "Time taken for testing one epoch: 6.20s\n",
      "Epoch 2 test loss: 0.42, acc: 0.47, f1: 0.03, mAP: 0.18\n",
      "Training (resnet50 no-pretraining no-warmup) finished in: 127.12 seconds\n",
      "Results saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/02_results/model-experiments.csv\n",
      "Model saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/models/resnet50-0.weights.h5\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-train_loss.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-train_acc.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-train_f1.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-train_map.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-train_subset_acc.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-test_loss.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-test_acc.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-test_f1.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-test_map.csv\n",
      "History saved to /Users/sheena/Development/master-dev/semester-2/mai-object-recognition/practicals/p1/data/01_histories/resnet50-0-test_subset_acc.csv\n"
     ]
    }
   ],
   "source": [
    "# Run model experiments\n",
    "exp_name = \"model-experiments\"\n",
    "for exp in experiments[exp_name]:\n",
    "    print(f\"Defining model: {exp.title}\")\n",
    "\n",
    "    # Select the corresponding network class\n",
    "    mynet = getattr(getattr(app, exp.net_name[0]), exp.net_name[1])\n",
    "\n",
    "    # Create the base pre-trained model\n",
    "    base_model = (\n",
    "        mynet(include_top=False)\n",
    "        if exp.train_from_scratch\n",
    "        else mynet(weights=\"imagenet\", include_top=False)\n",
    "    )\n",
    "\n",
    "    # Apply warm-up strategy\n",
    "    base_model.trainable = not exp.warm_up\n",
    "\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)  # Fully connected layer\n",
    "    predictions = Dense(num_classes, activation=exp.last_layer_activation)(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Define optimizers\n",
    "    warmup_optimizer = optimizers.RMSprop(learning_rate=exp.learning_rate * 0.1)\n",
    "    opt_rms = optimizers.RMSprop(learning_rate=exp.learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=exp.loss,\n",
    "        optimizer=warmup_optimizer if exp.warm_up else opt_rms,\n",
    "        metrics=[\"AUC\", f1_metric, mean_average_precision, subset_accuracy_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[exp.batch_size]\n",
    "    test_dataset = test_datasets[exp.batch_size]\n",
    "\n",
    "    train_and_test(\n",
    "        model, exp_name, exp, train_dataset, test_dataset, train_list, test_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best experiment of the 9 model experiments\n",
    "\n",
    "df = pd.read_csv(RESULTS_DIR / f\"model-experiments.csv\")\n",
    "best_id = df.loc[df[\"Test mAP\"].idxmax(), \"ID\"]\n",
    "\n",
    "best_model_experiment_config = next(\n",
    "    exp for exp in experiments[\"model-experiments\"] if exp.id == best_id\n",
    ")\n",
    "\n",
    "best_model_experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter experiments\n",
    "\n",
    "exp_name = \"hyperparameter-experiments\"\n",
    "for exp in experiments[exp_name]:\n",
    "    print(f\"Defining model: {exp.title}\")\n",
    "    print(\n",
    "        f\"Reusing net_name: {best_model_experiment_config.net_name}, train_from_scratch: {best_model_experiment_config.train_from_scratch}, warm_up: {best_model_experiment_config.warm_up} from best model experiment\"\n",
    "    )\n",
    "    exp.net_name = best_model_experiment_config.net_name\n",
    "    exp.train_from_scratch = best_model_experiment_config.train_from_scratch\n",
    "    exp.warm_up = best_model_experiment_config.warm_up\n",
    "\n",
    "    # Select the corresponding network class\n",
    "    mynet = getattr(getattr(app, exp.net_name[0]), exp.net_name[1])\n",
    "\n",
    "    # Create the base pre-trained model\n",
    "    base_model = (\n",
    "        mynet(include_top=False)\n",
    "        if exp.train_from_scratch\n",
    "        else mynet(weights=\"imagenet\", include_top=False)\n",
    "    )\n",
    "\n",
    "    # Apply warm-up strategy\n",
    "    base_model.trainable = not exp.warm_up\n",
    "\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)  # Fully connected layer\n",
    "    predictions = Dense(num_classes, activation=exp.last_layer_activation)(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Define optimizers\n",
    "    warmup_optimizer = optimizers.RMSprop(learning_rate=exp.learning_rate * 0.1)\n",
    "    opt_rms = optimizers.RMSprop(learning_rate=exp.learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=exp.loss,\n",
    "        optimizer=warmup_optimizer if exp.warm_up else opt_rms,\n",
    "        metrics=[\"AUC\", f1_metric, mean_average_precision, subset_accuracy_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[exp.batch_size]\n",
    "    test_dataset = test_datasets[exp.batch_size]\n",
    "\n",
    "    train_and_test(\n",
    "        model, exp_name, exp, train_dataset, test_dataset, train_list, test_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best experiment of the 9 hyperparameter experiments\n",
    "\n",
    "df = pd.read_csv(RESULTS_DIR / f\"hyperparameter-experiments.csv\")\n",
    "best_id = df.loc[df[\"Test mAP\"].idxmax(), \"ID\"]\n",
    "\n",
    "best_hyperparameter_experiment_config = next(\n",
    "    exp for exp in experiments[\"hyperparameter-experiments\"] if exp.id == best_id\n",
    ")\n",
    "\n",
    "best_hyperparameter_experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run augmentation experiments\n",
    "\n",
    "exp_name = \"augmentation-experiments\"\n",
    "for exp in experiments[exp_name]:\n",
    "    print(f\"Defining model: {exp.title}\")\n",
    "    print(\n",
    "        f\"Reusing net_name: {best_hyperparameter_experiment_config.net_name}, train_from_scratch: {best_hyperparameter_experiment_config.train_from_scratch}, warm_up: {best_hyperparameter_experiment_config.warm_up} from best hyperparameter experiment\"\n",
    "    )\n",
    "    exp.net_name = best_hyperparameter_experiment_config.net_name\n",
    "    exp.train_from_scratch = best_hyperparameter_experiment_config.train_from_scratch\n",
    "    exp.warm_up = best_hyperparameter_experiment_config.warm_up\n",
    "    exp.batch_size = best_hyperparameter_experiment_config.batch_size\n",
    "    exp.learning_rate = best_hyperparameter_experiment_config.learning_rate\n",
    "    exp.loss = best_hyperparameter_experiment_config.loss\n",
    "    exp.last_layer_activation = (\n",
    "        best_hyperparameter_experiment_config.last_layer_activation\n",
    "    )\n",
    "\n",
    "    # Select the corresponding network class\n",
    "    mynet = getattr(getattr(app, exp.net_name[0]), exp.net_name[1])\n",
    "\n",
    "    # Create the base pre-trained model\n",
    "    base_model = (\n",
    "        mynet(include_top=False)\n",
    "        if exp.train_from_scratch\n",
    "        else mynet(weights=\"imagenet\", include_top=False)\n",
    "    )\n",
    "\n",
    "    # Apply warm-up strategy\n",
    "    base_model.trainable = not exp.warm_up\n",
    "\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)  # Fully connected layer\n",
    "    predictions = Dense(num_classes, activation=exp.last_layer_activation)(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Define optimizers\n",
    "    warmup_optimizer = optimizers.RMSprop(learning_rate=exp.learning_rate * 0.1)\n",
    "    opt_rms = optimizers.RMSprop(learning_rate=exp.learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=exp.loss,\n",
    "        optimizer=warmup_optimizer if exp.warm_up else opt_rms,\n",
    "        metrics=[\"AUC\", f1_metric, mean_average_precision, subset_accuracy_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[exp.batch_size]\n",
    "    test_dataset = test_datasets[exp.batch_size]\n",
    "\n",
    "    train_and_test(\n",
    "        model, exp_name, exp, train_dataset, test_dataset, train_list, test_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier head experiments\n",
    "\n",
    "exp_name = \"classfier_head_experiments\"\n",
    "for exp in experiments[exp_name]:\n",
    "    print(f\"Defining model: {exp.title}\")\n",
    "    print(\n",
    "        f\"Reusing net_name: {best_hyperparameter_experiment_config.net_name}, train_from_scratch: {best_hyperparameter_experiment_config.train_from_scratch}, warm_up: {best_hyperparameter_experiment_config.warm_up} from best hyperparameter experiment\"\n",
    "    )\n",
    "    exp.net_name = best_hyperparameter_experiment_config.net_name\n",
    "    exp.train_from_scratch = best_hyperparameter_experiment_config.train_from_scratch\n",
    "    exp.warm_up = best_hyperparameter_experiment_config.warm_up\n",
    "    exp.batch_size = best_hyperparameter_experiment_config.batch_size\n",
    "    exp.learning_rate = best_hyperparameter_experiment_config.learning_rate\n",
    "    exp.loss = best_hyperparameter_experiment_config.loss\n",
    "    exp.last_layer_activation = (\n",
    "        best_hyperparameter_experiment_config.last_layer_activation\n",
    "    )\n",
    "\n",
    "    # Select the corresponding network class\n",
    "    mynet = getattr(getattr(app, exp.net_name[0]), exp.net_name[1])\n",
    "\n",
    "    # Create the base pre-trained model\n",
    "    base_model = (\n",
    "        mynet(include_top=False)\n",
    "        if exp.train_from_scratch\n",
    "        else mynet(weights=\"imagenet\", include_top=False)\n",
    "    )\n",
    "\n",
    "    # Apply warm-up strategy\n",
    "    base_model.trainable = not exp.warm_up\n",
    "\n",
    "    # Add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)  # Fully connected layer\n",
    "    predictions = Dense(num_classes, activation=exp.last_layer_activation)(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Define optimizers\n",
    "    warmup_optimizer = optimizers.RMSprop(learning_rate=exp.learning_rate * 0.1)\n",
    "    opt_rms = optimizers.RMSprop(learning_rate=exp.learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=exp.loss,\n",
    "        optimizer=warmup_optimizer if exp.warm_up else opt_rms,\n",
    "        metrics=[\"AUC\", f1_metric, mean_average_precision, subset_accuracy_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[exp.batch_size]\n",
    "    test_dataset = test_datasets[exp.batch_size]\n",
    "\n",
    "    train_and_test(\n",
    "        model, exp_name, exp, train_dataset, test_dataset, train_list, test_list\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
