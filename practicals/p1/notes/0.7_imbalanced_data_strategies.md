# Improving Performance on Underrepresented Labels in Multi-Label Image Classification

## Overview
This plan ensures that each label appears in approximately the same number of images by:
1. **Data Augmentation with Image Duplication** – Generating new augmented copies of images that contain underrepresented labels.
2. **Class Weighting in Loss Function** – Adjusting the loss function to prioritize learning from underrepresented labels.

---

## 1. Data Augmentation with Image Duplication

### Goal
- Generate additional images containing underrepresented labels.
- Maintain multi-label relationships (i.e., multiple labels per image).

### Steps

1. **Analyze Label Distribution**  
   - Count occurrences of each label across images.
   - Identify underrepresented labels.
   - Compute how many more images are needed per label.

2. **Duplicate and Augment Images**  
   - Select images containing underrepresented labels.
   - Apply **data augmentation** to create new samples.
   - Add augmented samples back to the dataset.

3. **Ensure Balanced Dataset**  
   - Stop augmentation when each label has the desired number of images.
   - Shuffle dataset to prevent bias.

---

### Implementation (TensorFlow + Keras)
```python
import tensorflow as tf
import numpy as np
from collections import Counter

# Example: Dataset with multiple labels per image
(train_images, train_labels), _ = tf.keras.datasets.cifar10.load_data()
train_labels = np.random.randint(0, 2, size=(train_images.shape[0], 10))  # Example multi-labels

# Count occurrences of each label
label_counts = np.sum(train_labels, axis=0)
max_count = np.max(label_counts)

# Define augmentation function
def augment_image(image):
    augmentation = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.2),
        tf.keras.layers.RandomZoom(0.2),
        tf.keras.layers.RandomBrightness(0.2),
    ])
    return augmentation(image)

# Generate new images to balance dataset
augmented_images = []
augmented_labels = []

for i in range(train_images.shape[0]):
    image = train_images[i]
    labels = train_labels[i]
    
    # Compute how many more times this image needs to be duplicated
    max_needed = np.max(max_count / (label_counts + 1e-6))  # Avoid division by zero
    duplication_factor = int(max_needed * np.mean(labels))  # More duplication for underrepresented labels
    
    for _ in range(duplication_factor):
        new_image = augment_image(tf.image.convert_image_dtype(image, tf.float32))
        augmented_images.append(new_image.numpy())
        augmented_labels.append(labels)

# Convert to TensorFlow dataset
augmented_images = np.array(augmented_images)
augmented_labels = np.array(augmented_labels)

# Merge original and augmented data
final_images = np.concatenate([train_images, augmented_images])
final_labels = np.concatenate([train_labels, augmented_labels])

# Convert to tf.data format
dataset = tf.data.Dataset.from_tensor_slices((final_images, final_labels))
dataset = dataset.shuffle(10000).batch(32)

# Example Model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='sigmoid')  # Multi-label classification
])

# Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train with balanced dataset
model.fit(dataset, epochs=10)
```

---

## 2. Class Weighting in Loss Function

### Goal
- Adjust the loss function to prioritize learning from images containing underrepresented labels.
- Ensure that rare labels contribute equally to the learning process.

### Steps

1. **Compute Class Weights for Multi-Label Classification**  
   - Use **inverse frequency** for each label:
     \[
     \text{weight}_i = \frac{\text{total samples}}{\text{num labels} \times \text{label count}_i}
     \]

2. **Modify Loss Function to Use Weights**  
   - Use **weighted binary cross-entropy** instead of standard loss.

---

### Implementation (TensorFlow + Keras)
```python
from sklearn.utils.class_weight import compute_class_weight

# Compute class weights
num_labels = train_labels.shape[1]  # Number of classes
total_samples = train_labels.shape[0]

class_weights = np.array([
    total_samples / (num_labels * (np.sum(train_labels[:, i]) + 1e-6))  # Avoid division by zero
    for i in range(num_labels)
])

# Custom weighted binary cross-entropy loss
def weighted_binary_crossentropy(y_true, y_pred):
    loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    weights = tf.constant(class_weights, dtype=tf.float32)
    return tf.reduce_mean(loss * weights)

# Compile model with custom loss
model.compile(optimizer='adam', loss=weighted_binary_crossentropy, metrics=['accuracy'])

# Train model
model.fit(dataset, epochs=10)
```

---

## Summary

| Method | Goal | Key Steps |
|--------|------|----------|
| **Data Augmentation with Image Duplication** | Generate new images to balance dataset | Identify underrepresented labels → Duplicate & augment images → Merge back into dataset |
| **Class Weighting in Loss Function** | Adjust loss to prioritize rare labels | Compute per-label weights → Modify loss function → Train model with weighted loss |
