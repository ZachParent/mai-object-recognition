## [[00_OR_Practical_Sessions.pdf]]
- we should use colab, or a personal computer
- practical report
	- practical page limit: 10 pages
		- extra pages penalized
		- appendix is OK
	- quality of writing is important
	- justify with citations
### [[00_OR_Practical_Sessions.pdf#page=4&selection=0,0,0,39|Deliverable 1 - CNN architecture design]]
- ==multi-class vs multi-label==
	- because multiple possible outputs are possible, applying sigmoid on each possible output is better than softmax, which prefers a single output
- starter code is given
1. analyze the dataset for the distribution of labels
2. [[00_OR_Practical_Sessions.pdf#page=4|train and evaluate 3 architectures]] (ConvNeXtTiny is the newest)
	- we must perform image normalization, squeeze the values within a fixed range
	- do we need some warm-up? what does that mean?
		- one way is to freeze the pre-trained network and train just the head, then train the whole network
	- choose the best parameters and move forward
		- 128 might be too many for batch_size
	- Develop two relevant data augmentation algorithms [[00_OR_Practical_Sessions.pdf#page=6&selection=20,0,20,49|00_OR_Practical_Sessions, p.6]] could be using an existing algorithm. most important is that we know what we are doing
		- could be motivated by techniques from a paper, even if it doesn't improve our results
- include many results and images in the report
### [[00_OR_Practical_Sessions.pdf#page=7&selection=0,0,0,31|Deliverable 2 - fashion parsing]]
- we can ignore item parts and attributes
1. analyze dataset
2. choose 3 segmentation networks and train them
	- try several from MMSegmentation
		- Tune the learning rate and batch size
		- Apply the basic data augmentation like rotation, scaling, cropping, mosaic, cutout, mixup, etc,
		- Train based on 192 vs 384 px image resolution. Is there any relationship between the resolution and accuracy of each label?
	- then choose the best network and resolution and
		- discard the large/overrepresented labels and finetune the network
3. compare to YOLO
	- the bounding box from YOLO can be simplified to semantic segmentation
	- "we will see" the effect
4. discuss 2 ideas to improve results, referencing SOTA
### [[00_OR_Practical_Sessions.pdf#page=13&selection=0,0,0,47|Deliverable 3 - Body and cloth depth estimation]]
- data is available
- starter kit will be uploaded, quite complete
- important: resizing the depth
	- the subject's depth should be relative to itself
	- could consider the background as 0 (close to camera) or inf (far from camera)
- baseline code is UNET
	- we could use transformers instead, or a transformers UNET
- we will choose hyperparameters
- hints in the source
- we could use perceptual loss, helps the network understand the context
	- apply L1 loss between VGG and UNET depth estimation
	- $$L(I') = \sum{|I'-G|} + \sum{|N(I')-N(G)}|$$
	