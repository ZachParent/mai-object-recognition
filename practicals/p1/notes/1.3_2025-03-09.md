# Meeting notes

- Zach has added a new final experiment with all of the best pieces from the previous experiments, just to see what the "best of the best" can achieve

- All other experiments are already finished. Some comments about the results:
    - Small learning rates are significantly better
    - Ensemble head does slightly better than the default, but it might be random chance (not very significant)
    - The best mAP scores are aroung 0.9, while the best per-class F1 score is at 0.6, which is to be investigated
        - Sheena brings up that in research papers the mAP scores for Inception and ResNet are usually around 0.85-0.9, so our results are within reasonable range
    - Results are consistent with out predictions taken from the dataset analysis

- We need to figure out how to structure our results: only one section that summarizes everything and talks about the best runs, or 4 sections mirroring the experiments setup.
    - If we do 4 sections, it might get very lengthy.
    - Zach (in charge of plots) would like to know which plots are required for the results section (the person in charge of that is TBD yet)
    - Maybe we could show the mAP for all runs but then only delve into the rest of metrics for the best run.
    - Loss history plots for LR 
    - We can show predicted co-ocurrences vs true co-ocurrences to see if the model have truly learned what obejcts usually appear together

- Do we have all of the responsabilities in the report covered?
    - Only results has to be assigned (probably divided)
    - Zach is in charge of plots, so he would like to back off from the writing itself, and will provide what the rest need
    - Quantitative and qualitative data, and confusion matrices to validate results

- For the plots, we can go through the RQ an see how we visualize each:
    - RQ1: Look at the mAP scores of the 9 runs.
        - Grouped barplot
        - 3x3 Heatmap
    - RQ2: 
        - For LR, training curves for the best batch sample
        - For BS, show that 16 is too little, while 32 and 64 are similar
        - Maybe everything can be put into just 1 plot.
    - RQ3 and RQ4:
        - Data augmentation, maybe training curves to show reduced overfitting
        - Enhanced label balance, show other metrics (F1 score and/or precision), maybe per class. We can show before and after heatmaps (confusion matrix-like)
    - RQ5: 
        - Ensemble should be more consistent, and have less overfitting when compared to default -> Maybe train-test curves (default vs ensemble)
        - Attention should caputure co-ocurrences better -> subset accuracy, confusion matrix or something like that (if any)

- Zach is running augmentation and imbalance experiments that are expected to be done soon (imbalance might take more than expected) -> Might even be prohibitively long
    - We might have to get rid of the "All" imbalance experiment

- The Task subsection of the report might not be necessary.

- We will split the results section by RQs:
    - Sheena: RQ1 and RQ2
    - Pedro/Mart√≠: RQ3 and RQ4
    - Bruno RQ5

- Deadline:
    - Report done before 2025/03/09 1pm
    - 
