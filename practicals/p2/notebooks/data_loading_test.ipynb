{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\Documents\\MAI\\OR\\ass1\\mai-object-recognition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Main Clothing Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_item_names = [\n",
    "    'shirt, blouse',\n",
    "    'top, t-shirt, sweatshirt',\n",
    "    'sweater',\n",
    "    'cardigan',\n",
    "    'jacket',\n",
    "    'vest',\n",
    "    'pants',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    'coat',\n",
    "    'dress',\n",
    "    'jumpsuit',\n",
    "    'cape',\n",
    "    'glasses',\n",
    "    'hat',\n",
    "    'headband, head covering, hair accessory',\n",
    "    'tie',\n",
    "    'glove',\n",
    "    'watch',\n",
    "    'belt',\n",
    "    'leg warmer',\n",
    "    'tights, stockings',\n",
    "    'sock',\n",
    "    'shoe',\n",
    "    'bag, wallet',\n",
    "    'scarf',\n",
    "    'umbrella'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"detection-datasets/fashionpedia\", split=\"train\")\n",
    "\n",
    "# Split into 90% training and 10% validation.\n",
    "num_total = len(dataset)\n",
    "train_dataset = dataset.select(range(int(0.9 * num_total)))\n",
    "val_dataset = dataset.select(range(int(0.9 * num_total), num_total))\n",
    "\n",
    "# Get the official label list from the dataset (for the \"objects.category\" field).\n",
    "label_list = dataset.features[\"objects\"].feature[\"category\"].names\n",
    "# Map the main item names to their numeric IDs.\n",
    "main_item_ids = [label_list.index(name) for name in main_item_names if name in label_list]\n",
    "print(\"Main item IDs:\", main_item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Filter Annotations for Main Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_main_item(example):\n",
    "    \"\"\"Keep samples with at least one bounding box having a main item category.\"\"\"\n",
    "    categories = example[\"objects\"][\"category\"]\n",
    "    return any(cat in main_item_ids for cat in categories)\n",
    "\n",
    "def filter_main_items(example):\n",
    "    \"\"\"Remove bounding boxes and related annotations that are not main items.\"\"\"\n",
    "    old_cats = example[\"objects\"][\"category\"]\n",
    "    old_bboxes = example[\"objects\"][\"bbox\"]\n",
    "    old_area = example[\"objects\"][\"area\"]\n",
    "    new_cats, new_bboxes, new_area = [], [], []\n",
    "    for cat, bb, area in zip(old_cats, old_bboxes, old_area):\n",
    "        if cat in main_item_ids:\n",
    "            new_cats.append(cat)\n",
    "            new_bboxes.append(bb)\n",
    "            new_area.append(area)\n",
    "    example[\"objects\"][\"category\"] = new_cats\n",
    "    example[\"objects\"][\"bbox\"] = new_bboxes\n",
    "    example[\"objects\"][\"area\"] = new_area\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only images with at least one main item.\n",
    "train_dataset = train_dataset.filter(has_main_item)\n",
    "val_dataset = val_dataset.filter(has_main_item)\n",
    "\n",
    "# Map each sample to keep only the main item annotations.\n",
    "train_dataset = train_dataset.map(filter_main_items)\n",
    "val_dataset = val_dataset.map(filter_main_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Augmentation & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sample(example):\n",
    "    \"\"\"\n",
    "    For training: apply random horizontal flip (with bbox adjustment),\n",
    "    color jitter, resize to a fixed size, and convert to tensor.\n",
    "    \"\"\"\n",
    "    # Ensure image is a PIL image.\n",
    "    image = example[\"image\"]\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    orig_width, orig_height = image.size\n",
    "\n",
    "    # Random horizontal flip with probability 0.5.\n",
    "    if random.random() < 0.5:\n",
    "        image = ImageOps.mirror(image)\n",
    "        new_bboxes = []\n",
    "        for bb in example[\"objects\"][\"bbox\"]:\n",
    "            x1, y1, x2, y2 = bb\n",
    "            # Flip bbox horizontally.\n",
    "            new_bb = [orig_width - x2, y1, orig_width - x1, y2]\n",
    "            new_bboxes.append(new_bb)\n",
    "        example[\"objects\"][\"bbox\"] = new_bboxes\n",
    "\n",
    "    # Define augmentation transforms: color jitter, resize and conversion to tensor.\n",
    "    aug_transforms = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = aug_transforms(image)\n",
    "    example[\"image\"] = image_tensor\n",
    "\n",
    "    # Update bounding boxes to the new image size.\n",
    "    new_width, new_height = 224, 224\n",
    "    scale_x = new_width / orig_width\n",
    "    scale_y = new_height / orig_height\n",
    "    new_bboxes = []\n",
    "    for bb in example[\"objects\"][\"bbox\"]:\n",
    "        x1, y1, x2, y2 = bb\n",
    "        new_bb = [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n",
    "        new_bboxes.append(new_bb)\n",
    "    example[\"objects\"][\"bbox\"] = new_bboxes\n",
    "\n",
    "    return example\n",
    "\n",
    "def transform_sample(example):\n",
    "    \"\"\"\n",
    "    For validation: apply only resizing and conversion to tensor.\n",
    "    \"\"\"\n",
    "    image = example[\"image\"]\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    orig_width, orig_height = image.size\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = trans(image)\n",
    "    example[\"image\"] = image_tensor\n",
    "\n",
    "    new_width, new_height = 224, 224\n",
    "    scale_x = new_width / orig_width\n",
    "    scale_y = new_height / orig_height\n",
    "    new_bboxes = []\n",
    "    for bb in example[\"objects\"][\"bbox\"]:\n",
    "        x1, y1, x2, y2 = bb\n",
    "        new_bb = [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n",
    "        new_bboxes.append(new_bb)\n",
    "    example[\"objects\"][\"bbox\"] = new_bboxes\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentations to training set and a simpler transform to validation set.\n",
    "train_dataset = train_dataset.map(augment_sample)\n",
    "val_dataset = val_dataset.map(transform_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a Custom Collate Function for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate_fn to stack images (which have uniform shape after resizing)\n",
    "    and leave bounding boxes and labels as lists (since they vary in number per image).\n",
    "    \"\"\"\n",
    "    images = torch.stack([sample[\"image\"] for sample in batch])\n",
    "    # Each sample's bounding boxes and category labels remain as lists.\n",
    "    bboxes = [torch.tensor(sample[\"objects\"][\"bbox\"], dtype=torch.float32) for sample in batch]\n",
    "    labels = [torch.tensor(sample[\"objects\"][\"category\"], dtype=torch.int64) for sample in batch]\n",
    "    \n",
    "    return {\n",
    "        \"images\": images,  # Tensor of shape (batch_size, C, H, W)\n",
    "        \"bboxes\": bboxes,  # List of tensors (variable length per image)\n",
    "        \"labels\": labels   # List of tensors (variable length per image)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(\"Train Batch Images Shape:\", batch[\"images\"].shape)  # Expected: (4, C, 224, 224)\n",
    "    print(\"Number of Bounding Box Sets in Batch:\", len(batch[\"bboxes\"]))\n",
    "    # Each entry in batch[\"bboxes\"] is a tensor of shape (num_objects, 4)\n",
    "    break\n",
    "\n",
    "for batch in val_loader:\n",
    "    print(\"Validation Batch Images Shape:\", batch[\"images\"].shape)\n",
    "    print(\"Number of Bounding Box Sets in Batch:\", len(batch[\"bboxes\"]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
