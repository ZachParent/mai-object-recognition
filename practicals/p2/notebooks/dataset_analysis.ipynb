{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pypalettes import load_cmap\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "project_root = Path().resolve().parent \n",
    "src_dir = project_root / \"src\"\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from config import TRAIN_ANNOTATIONS_JSON, VAL_ANNOTATIONS_JSON, FIGURES_DIR\n",
    "from dataset import load_category_mappings, create_segmentation_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(ann_file, dataset_name):\n",
    "    with open(ann_file, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    coco = COCO(ann_file)\n",
    "    category_mappings = load_category_mappings(ann_file)\n",
    "    image_data = []\n",
    "\n",
    "    for img in dataset[\"images\"]:\n",
    "        img_id = img[\"id\"]\n",
    "        img_pixels = img[\"height\"] * img[\"width\"]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        mask = create_segmentation_mask(\n",
    "            coco, img_id, img[\"height\"], img[\"width\"], category_mappings\n",
    "        )\n",
    "        total_foreground_pixels = np.count_nonzero(mask)\n",
    "        \n",
    "        for ann in anns:\n",
    "            cat_id = ann[\"category_id\"]\n",
    "            cat_name = coco.cats[cat_id][\"name\"]\n",
    "            fg_pixels = ann.get(\"area\", 0)\n",
    "\n",
    "            image_data.append(\n",
    "                {\n",
    "                    \"image_id\": img_id,\n",
    "                    \"image_label\": img[\"file_name\"],\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"category_name\": cat_name,\n",
    "                    \"category_pixels\": fg_pixels,\n",
    "                    \"foreground_pixels\": total_foreground_pixels,\n",
    "                    \"image_pixels\": img_pixels,\n",
    "                    \"fg_area_ratio\": fg_pixels / img_pixels if img_pixels > 0 else 0,\n",
    "                    \"total_area_ratio\": total_foreground_pixels / img_pixels\n",
    "                    if img_pixels > 0\n",
    "                    else 0,\n",
    "                    \"dataset\": dataset_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(image_data)\n",
    "\n",
    "train_df = load_annotations(TRAIN_ANNOTATIONS_JSON, \"train\")\n",
    "val_df = load_annotations(VAL_ANNOTATIONS_JSON, \"test\")\n",
    "\n",
    "dataset_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of images per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_item_names = [\n",
    "    \"shirt, blouse\",\n",
    "    \"top, t-shirt, sweatshirt\",\n",
    "    \"sweater\",\n",
    "    \"cardigan\",\n",
    "    \"jacket\",\n",
    "    \"vest\",\n",
    "    \"pants\",\n",
    "    \"shorts\",\n",
    "    \"skirt\",\n",
    "    \"coat\",\n",
    "    \"dress\",\n",
    "    \"jumpsuit\",\n",
    "    \"cape\",\n",
    "    \"glasses\",\n",
    "    \"hat\",\n",
    "    \"headband, head covering, hair accessory\",\n",
    "    \"tie\",\n",
    "    \"glove\",\n",
    "    \"watch\",\n",
    "    \"belt\",\n",
    "    \"leg warmer\",\n",
    "    \"tights, stockings\",\n",
    "    \"sock\",\n",
    "    \"shoe\",\n",
    "    \"bag, wallet\",\n",
    "    \"scarf\",\n",
    "    \"umbrella\",\n",
    "]\n",
    "\n",
    "count_df = (\n",
    "    dataset_df.groupby([\"category_id\", \"category_name\"])[\"image_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"occurrence\")\n",
    ")\n",
    "\n",
    "count_df[\"label_count\"] = count_df[\"occurrence\"].apply(len)\n",
    "count_df[\"image_count\"] = count_df[\"occurrence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "count_df = count_df[count_df[\"category_name\"].isin(main_item_names)]\n",
    "\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = load_cmap(\"Flash\", cmap_type=\"continuous\", reverse=True)\n",
    "\n",
    "num_categories = len(count_df)\n",
    "\n",
    "color_values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "colors = [cmap(value) for value in color_values]\n",
    "\n",
    "sorted_stats = count_df.sort_values(\"label_count\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "max_label_count = sorted_stats[\"label_count\"].max()\n",
    "max_image_count = sorted_stats[\"image_count\"].max()\n",
    "max_count = max(max_label_count, max_image_count)\n",
    "\n",
    "plt.ylim(100, max_count * 1.2)\n",
    "\n",
    "bars1 = plt.bar(\n",
    "    range(len(sorted_stats)),\n",
    "    sorted_stats[\"image_count\"],\n",
    "    color=colors,\n",
    "    label=\"Image Count\",\n",
    ")\n",
    "\n",
    "bars2 = plt.bar(\n",
    "    range(len(sorted_stats)),\n",
    "    sorted_stats[\"label_count\"],\n",
    "    alpha=0.5,\n",
    "    color=colors,\n",
    "    label=\"Total Occurences\",\n",
    ")\n",
    "\n",
    "plt.xticks(\n",
    "    range(len(sorted_stats)), sorted_stats[\"category_name\"], rotation=45, ha=\"right\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Category\", fontsize=16, fontweight=\"bold\")\n",
    "plt.ylabel(\"Count\", fontsize=16, fontweight=\"bold\")\n",
    "plt.title(f\"Overall Category Counts by Image\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(-0.5, len(sorted_stats) - 0.5)\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        max(height - 10,125),\n",
    "        f\"{int(height)}\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        color=\"white\",\n",
    "        bbox=dict(facecolor=\"black\", alpha=0.5, pad=1),\n",
    "    )\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom')\n",
    "    \n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"category_counts_by_image.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset_df[dataset_df[\"dataset\"] == \"train\"]\n",
    "test_df = dataset_df[dataset_df[\"dataset\"] == \"test\"]\n",
    "\n",
    "train_count_df = (\n",
    "    train_df.groupby([\"category_id\", \"category_name\"])[\"image_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"occurrence\")\n",
    ")\n",
    "\n",
    "train_count_df[\"label_count\"] = train_count_df[\"occurrence\"].apply(len)\n",
    "train_count_df[\"image_count\"] = train_count_df[\"occurrence\"].apply(\n",
    "    lambda x: len(set(x))\n",
    ")\n",
    "\n",
    "train_count_df = train_count_df[train_count_df[\"category_name\"].isin(main_item_names)]\n",
    "\n",
    "test_count_df = (\n",
    "    test_df.groupby([\"category_id\", \"category_name\"])[\"image_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"occurrence\")\n",
    ")\n",
    "\n",
    "test_count_df[\"label_count\"] = test_count_df[\"occurrence\"].apply(len)\n",
    "test_count_df[\"image_count\"] = test_count_df[\"occurrence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "test_count_df = test_count_df[test_count_df[\"category_name\"].isin(main_item_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = count_df.sort_values(\"image_count\", ascending=False)\n",
    "\n",
    "categories = sorted_df[\"category_name\"]\n",
    "train_counts = train_count_df.set_index(\"category_name\").loc[categories, \"image_count\"]\n",
    "test_counts = test_count_df.set_index(\"category_name\").loc[categories, \"image_count\"]\n",
    "\n",
    "train_proportions = train_counts / train_counts.sum()\n",
    "test_proportions = test_counts / test_counts.sum()\n",
    "\n",
    "test_color = \"#CD64B5\"\n",
    "train_color = \"#680468\"  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bar_width = 0.6\n",
    "x = np.arange(len(categories))  \n",
    "\n",
    "ax.bar(\n",
    "    x,\n",
    "    train_proportions,\n",
    "    color=train_color,\n",
    "    label=\"Train\",\n",
    "    width=bar_width,\n",
    "    align=\"center\",\n",
    ")\n",
    "ax.bar(\n",
    "    x,\n",
    "    test_proportions,\n",
    "    color=test_color,\n",
    "    bottom=train_proportions,\n",
    "    label=\"Test\",\n",
    "    width=bar_width,\n",
    "    alpha=0.8,\n",
    "    align=\"center\",\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_title(\n",
    "    \"Category Proportions in Train and Test Sets\", fontsize=18, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "ax.set_xlim(-0.5, len(categories) - 0.5)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"train_test_category_proportions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations  \n",
    "\n",
    "The Fashionpedia dataset reveals that certain clothing items are significantly more common than others. Among the most frequently occurring labels, \"shoe\" appears in 24,758 images, followed by \"dress\" in 19,172 images and \"top, t-shirt, sweatshirt\" in 16,639 images. In contrast, some labels, such as \"cape,\" \"umbrella,\" and \"leg warmer,\" are far less prevalent.  \n",
    "\n",
    "Additionally, many labels appear multiple times within a single image. This pattern is particularly evident for items such as \"shoe,\" \"tights, stockings,\" \"sock,\" \"glove,\" and \"leg warmer,\" as these typically come in pairs. Accessories also frequently appear in multiples within an image, which is expected since outfits often include more than one accessory.  \n",
    "\n",
    "When looking at the train / test ratios of in how many images a category occurred / the total number of images, one can observe that most categories are represented at a similar frequency in both the train and test datasets. However, **tie** has a much higher ratio in the train dataset, while **leg warmer** has a much higher ratio in the test dataset. Other categories, such as **cardigan**, **sweater**, and **shirt/blouse**, also show noticeable differences, with higher proportions in the training set compared to the test set.  \n",
    "\n",
    "### Hypothesis  \n",
    "\n",
    "Based on these observations, we hypothesize the following regarding the performance of our clothing segmentation model:  \n",
    "\n",
    "- **Higher segmentation accuracy** is expected for **frequent categories** (e.g., shoes, dresses, tops) due to the larger volume of training data available for these items.  \n",
    "- **Lower segmentation accuracy** may occur for **rare categories** (e.g., capes, umbrellas, leg warmers) due to limited training samples, leading to poor generalization.  \n",
    "- **Items that appear in multiples within a single image** (e.g., shoes, socks, gloves) may pose a challenge for segmentation, as distinguishing and accurately identifying multiple instances of the same item could be difficult.  \n",
    "- **Accessory segmentation performance** may vary depending on their size and placement within an image, as small or overlapping accessories could be harder for the model to detect.  \n",
    "- The model may **struggle to generalize well for categories like leg warmer**, as it is underrepresented in the training set but more common in the test set.  \n",
    "- Categories like **tie**, **cardigan**, and **sweater** may perform better during training but could face overfitting issues, leading to reduced performance on the test set.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Variance for the Pixel Ratio per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df = (\n",
    "    dataset_df.groupby([\"category_id\", \"category_name\"])\n",
    "    .agg(\n",
    "        area_ratio_mean=(\"total_area_ratio\", \"mean\"),\n",
    "        area_ratio_median=(\"total_area_ratio\", \"median\"),\n",
    "        area_ratio_stdev=(\"total_area_ratio\", \"std\"),\n",
    "        foreground_area_ratio_mean=(\"fg_area_ratio\", \"mean\"),\n",
    "        foreground_area_ratio_median=(\"fg_area_ratio\", \"median\"),\n",
    "        foreground_area_ratio_stdev=(\"fg_area_ratio\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ratio_df = ratio_df[ratio_df[\"category_name\"].isin(main_item_names)]\n",
    "\n",
    "ratio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stats = ratio_df.sort_values(\"area_ratio_mean\", ascending=False)\n",
    "\n",
    "cmap = load_cmap(\"green_material\", cmap_type=\"continuous\", reverse=True)\n",
    "\n",
    "num_categories = len(sorted_stats)\n",
    "\n",
    "color_values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "colors = [cmap(value) for value in color_values]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8  \n",
    "x_positions = np.arange(len(sorted_stats))\n",
    "\n",
    "plt.bar(x_positions, sorted_stats[\"area_ratio_mean\"], color=colors, width=bar_width)\n",
    "\n",
    "plt.errorbar(\n",
    "    x_positions,\n",
    "    sorted_stats[\"area_ratio_mean\"],\n",
    "    yerr=sorted_stats[\"area_ratio_stdev\"],\n",
    "    ecolor=\"#333333\",\n",
    "    capsize=5,\n",
    "    fmt=\"none\",\n",
    "    label=\"Standard Deviation from Mean\",\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    y=sorted_stats[\"area_ratio_mean\"],\n",
    "    xmin=x_positions - 0.4,\n",
    "    xmax=x_positions + 0.38,\n",
    "    colors=\"brown\",\n",
    "    linewidth=2,\n",
    "    label=\"Mean\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "plt.xticks(x_positions, sorted_stats[\"category_name\"], rotation=45, ha=\"right\")\n",
    "\n",
    "plt.xlim(-0.5, len(sorted_stats) - 0.5) \n",
    "\n",
    "plt.xlabel(\"Category\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Area Ratio\", fontsize=14, fontweight=\"bold\")\n",
    "plt.title(\"Mean and Median Area Ratio by Category\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "plt.scatter(\n",
    "    x_positions,\n",
    "    sorted_stats[\"area_ratio_median\"],\n",
    "    marker=\"o\",\n",
    "    color=\"lightblue\",\n",
    "    edgecolor=\"blue\",\n",
    "    linewidth=1.5,\n",
    "    s=100,\n",
    "    zorder=4,\n",
    "    label=\"Median\",\n",
    ")\n",
    "\n",
    "upper_limit = (\n",
    "    sorted_stats[\"area_ratio_mean\"] + sorted_stats[\"area_ratio_stdev\"]\n",
    ").max() + 0.05\n",
    "plt.ylim(0, upper_limit)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"category_area_ratio.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stats = ratio_df.sort_values(\n",
    "    \"foreground_area_ratio_mean\", ascending=False\n",
    ")\n",
    "\n",
    "cmap = load_cmap(\"blue_material\", cmap_type=\"continuous\", reverse=True)\n",
    "\n",
    "num_categories = len(sorted_stats)\n",
    "\n",
    "color_values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "colors = [cmap(value) for value in color_values]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "x_positions = np.arange(len(sorted_stats))\n",
    "\n",
    "plt.bar(x_positions, sorted_stats[\"foreground_area_ratio_mean\"], color=colors, width=bar_width)\n",
    "\n",
    "plt.errorbar(\n",
    "    x_positions,\n",
    "    sorted_stats[\"foreground_area_ratio_mean\"],\n",
    "    yerr=sorted_stats[\"foreground_area_ratio_stdev\"],\n",
    "    ecolor=\"#333333\",\n",
    "    capsize=5,\n",
    "    fmt=\"none\",\n",
    "    label=\"Standard Deviation from Mean\",\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    y=sorted_stats[\"foreground_area_ratio_median\"],\n",
    "    xmin=x_positions - 0.4,\n",
    "    xmax=x_positions + 0.38,\n",
    "    colors=\"brown\",\n",
    "    linewidth=2,\n",
    "    label=\"Mean\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "plt.xticks(x_positions, sorted_stats[\"category_name\"], rotation=45, ha=\"right\")\n",
    "\n",
    "plt.xlim(-0.5, len(sorted_stats) - 0.5)\n",
    "\n",
    "plt.xlabel(\"Category\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Foreground Area Ratio\", fontsize=14, fontweight=\"bold\")\n",
    "plt.title(\"Mean and Median Foreground Area Ratio by Category\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "plt.scatter(\n",
    "    x_positions,\n",
    "    sorted_stats[\"foreground_area_ratio_median\"],\n",
    "    marker=\"o\",\n",
    "    color=\"lightblue\",\n",
    "    edgecolor=\"blue\",\n",
    "    linewidth=1.5,\n",
    "    s=100,\n",
    "    zorder=4,\n",
    "    label=\"Median\",\n",
    ")\n",
    "\n",
    "upper_limit = (\n",
    "    sorted_stats[\"foreground_area_ratio_mean\"]\n",
    "    + sorted_stats[\"foreground_area_ratio_stdev\"]\n",
    ").max() + 0.05\n",
    "plt.ylim(0, upper_limit)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"category_foreground_area_ratio.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations  \n",
    "\n",
    "In our analysis, we examined two ratios for each clothing item:  \n",
    "\n",
    "1. **Overall Image Ratio**:  \n",
    "     $$ \\frac{\\# \\text{ pixels of label } j \\text{ in image } i}{\\# \\text{ pixels in image } i} $$  \n",
    " \n",
    "2. **Foreground Ratio** (excluding the background):  \n",
    "   $$ \\frac{\\# \\text{ pixels of label } j \\text{ in image } i}{\\# \\text{ foreground pixels in image } i} $$  \n",
    "\n",
    "From this analysis, we found that clothing items such as **sweaters, cardigans, and coats** had the highest ratio in both metrics. This indicates that these garments often dominate the image composition as well as the foreground, likely because they cover large portions of the body and are prominently displayed in fashion images.  \n",
    "\n",
    "Interestingly, while the lowest ratio in the **overall image calculation** belonged to **shorts, shoes, and socks**, the lowest **foreground ratio** was observed for **glasses, socks, and watches**.  \n",
    "\n",
    "This suggests that smaller accessories and footwear contribute minimally to the overall scene, but when considering only the foreground, accessories such as glasses and watches are even less prominent. This discrepancy could be explained by the fact that:  \n",
    "\n",
    "- **Shoes and socks**, while relatively small, still appear in the lower part of the image and are often visible in full-body shots.  \n",
    "- **Accessories like glasses and watches**, though present in the foreground, occupy significantly fewer pixels relative to other clothing items, making them harder to detect.  \n",
    "\n",
    "Regarding variance, we observed that while standard deviations for the **overall image ratio** were all large and similar across categories, the **foreground ratio** exhibited more variability. Notably, **tops, t-shirts, sweatshirts, vests, and headwear (including headbands, head coverings, and hair accessories)** showed relatively high standard deviation.  \n",
    "\n",
    "This likely arises because these categories are highly dependent on the specific image composition, pose, and framingâ€”sometimes occupying large portions of the image (e.g., in upper-body shots) and sometimes much less (e.g., in full-body shots).  \n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "Based on these observations, we can make the following predictions about how our clothing image segmentation model will perform across different categories:  \n",
    "\n",
    "- **High Coverage Items (Sweaters, Cardigans, Coats):**  \n",
    "  These items dominate both the image and the foreground, making them easier to segment with high accuracy. The model should perform well here due to their size and clear boundaries.  \n",
    "\n",
    "- **Low Coverage Items in Overall Image (Shorts, Shoes, Socks):**  \n",
    "  These items occupy smaller portions of the image, which might lead to challenges in detection, particularly if they are close in color to the background or blend with other clothing.  \n",
    "\n",
    "- **Low Coverage Items in Foreground (Glasses, Socks, Watches):**  \n",
    "  Since these objects occupy an even smaller percentage of the foreground, segmentation performance may suffer. Accessories like glasses and watches, in particular, might be harder to detect due to their fine details and potential occlusion by hair or sleeves.  \n",
    "\n",
    "- **High Variability Items (Tops, T-Shirts, Sweatshirts, Vests, Headwear):**  \n",
    "  The segmentation model might struggle with consistency due to their varying sizes in different image compositions. Performance could fluctuate depending on how prominently they appear in the scene.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
